# Local-AI. Offline First AI for Everyone

> Run AI models entirely on your own device, without cloud, subscriptions, or internet access.

## Overview

**Local-AI** is an open-source project that empowers users to run powerful language models (LLMs) on their local devices â€” even with limited computing power. Whether you're working on a feature phone, an older CPU-based laptop, or deploying to rural areas with unreliable connectivity, Local-AI gives you full control over your data and models.

This project uses the following stack:

- [Ollama](https://ollama.com) â€” to run and customize LLMs on macOS, Linux, and Windows
- [Open Web UI](https://github.com/open-webui/open-webui) â€” a browser-based chat interface, offline and open-source
- [Docker](https://www.docker.com) â€” for simple and reproducible deployments

---

## Features

- âœ… **Run Offline** â€” Works without internet access or cloud services
- ğŸ” **100% Private** â€” Keeps all data on your device
- ğŸ§© **Customizable** â€” Fine-tune and personalize your own AI models
- ğŸ’¸ **Free and Open Source** â€” No subscriptions or vendor lock-in
- ğŸ’» **Low Resource Compatible** â€” Works on CPUs, no GPU required

---

## ğŸš€ Getting Started

### 1. ğŸ“¦ Install Ollama

Download Ollama from: https://ollama.com/download  
Or use the terminal:

```bash
# macOS
/bin/bash -c "$(curl -fsSL https://ollama.com/install.sh)"

